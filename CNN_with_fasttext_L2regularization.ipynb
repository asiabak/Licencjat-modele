{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOF7RlMF/vGosybDRXwHEJJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asiabak/Licencjat-modele/blob/main/CNN_with_fasttext_L2regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim tensorflow pandas==2.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IFywlG35YzD0",
        "outputId": "1030bea4-2d19-42fb-e80d-693bb1d2c054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Collecting numpy>=1.23.2 (from pandas==2.2.2)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXY3oeeZXYRP",
        "outputId": "d75554a8-543f-4f84-fdb1-655d31f83d99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CNN on reviews dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.4576 - loss: 1.0892 - val_accuracy: 0.5245 - val_loss: 1.0529\n",
            "Epoch 2/5\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.4746 - loss: 1.0586 - val_accuracy: 0.5245 - val_loss: 1.0454\n",
            "Epoch 3/5\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4861 - loss: 1.0243 - val_accuracy: 0.5245 - val_loss: 1.0543\n",
            "Epoch 4/5\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4896 - loss: 1.0114 - val_accuracy: 0.3817 - val_loss: 1.0675\n",
            "Epoch 5/5\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5451 - loss: 0.9672 - val_accuracy: 0.5203 - val_loss: 1.0872\n",
            "CNN training completed in 15.20 seconds\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
            "Accuracy for CNN model trained on reviews only: 0.4968\n",
            "Training CNN on reviews + idioms dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.4469 - loss: 1.0952 - val_accuracy: 0.4896 - val_loss: 1.0642\n",
            "Epoch 2/5\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4979 - loss: 1.0579 - val_accuracy: 0.4915 - val_loss: 1.0541\n",
            "Epoch 3/5\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5180 - loss: 1.0255 - val_accuracy: 0.4915 - val_loss: 1.0572\n",
            "Epoch 4/5\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5404 - loss: 0.9609 - val_accuracy: 0.4915 - val_loss: 1.0931\n",
            "Epoch 5/5\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5984 - loss: 0.9115 - val_accuracy: 0.4877 - val_loss: 1.1130\n",
            "CNN training completed in 7.82 seconds\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "Accuracy for CNN model trained on reviews + idioms: 0.5410\n",
            "\n",
            "Model Performance Comparison:\n",
            "Reviews Only:        0.4968\n",
            "Reviews with Idioms: 0.5410\n",
            "Improvement:         4.42%\n"
          ]
        }
      ],
      "source": [
        "# The initial FastText model loading and text preprocessing remains the same\n",
        "# First, verify GPU is available in Colab\n",
        "import tensorflow as tf\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Install necessary libraries if not already available\n",
        "# !pip install gensim scikit-learn nltk pandas\n",
        "\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import FastText\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Input, Dropout, Flatten, MaxPooling1D\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Use GPU for tensor operations\n",
        "with tf.device('/GPU:0'):\n",
        "    # Download NLTK data if needed\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "    # Set up paths for KGR10 FastText model\n",
        "    model_url = \"https://huggingface.co/clarin-pl/fasttext-kgr10/resolve/main/kgr10.plain.skipgram.dim100.neg10.bin\"\n",
        "    model_path = \"kgr10.plain.skipgram.dim100.neg10.bin\"\n",
        "\n",
        "    # Download model if it doesn't exist\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Downloading KGR10 FastText model from {model_url}...\")\n",
        "        urllib.request.urlretrieve(model_url, model_path)\n",
        "        print(\"Download complete!\")\n",
        "    else:\n",
        "        print(f\"Using existing model at {model_path}\")\n",
        "\n",
        "    # Load the FastText model\n",
        "    print(\"Loading KGR10 FastText model...\")\n",
        "    model = FastText.load_fasttext_format(model_path)\n",
        "    print(f\"Model loaded! Vector size: {model.vector_size}\")\n",
        "\n",
        "    # Load datasets\n",
        "    print(\"Loading datasets...\")\n",
        "    reviews = pd.read_csv(\"filmweb_jednolity_sentyment.csv\")\n",
        "    reviews_with_idioms = pd.read_csv('filmweb_i_idiomy.csv')\n",
        "    print(f\"Loaded {len(reviews)} reviews and {len(reviews_with_idioms)} reviews with idioms\")\n",
        "\n",
        "    # Text preprocessing\n",
        "    def preprocess_text(text):\n",
        "        # Handle NaN values\n",
        "        if isinstance(text, float) and np.isnan(text):\n",
        "            return \"\"\n",
        "\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        words = text.split()\n",
        "        return ' '.join(words)\n",
        "\n",
        "    # Apply text preprocessing\n",
        "    print(\"Preprocessing text...\")\n",
        "    reviews['review_processed'] = reviews['review'].apply(preprocess_text)\n",
        "    reviews_with_idioms['review_processed'] = reviews_with_idioms['review'].apply(preprocess_text)\n",
        "\n",
        "    # Keep the same sequence vectorization approach for CNN\n",
        "    def create_sequence_vectors(texts, model, max_length=100):\n",
        "        # Get sequence of vectors for each text\n",
        "        def get_sequence_embeddings(text):\n",
        "            if not text:  # Handle empty strings\n",
        "                return [np.zeros(model.vector_size)]\n",
        "\n",
        "            words = word_tokenize(text.lower())\n",
        "            word_vectors = []\n",
        "            for word in words[:max_length]:  # Limit sequence length\n",
        "                try:\n",
        "                    # Get vector for the word\n",
        "                    word_vectors.append(model.wv[word])\n",
        "                except KeyError:\n",
        "                    # Skip words not in vocabulary\n",
        "                    continue\n",
        "\n",
        "            if not word_vectors:\n",
        "                return [np.zeros(model.vector_size)]\n",
        "            return word_vectors\n",
        "\n",
        "        # Process each text in the batch\n",
        "        sequences = [get_sequence_embeddings(text) for text in texts]\n",
        "\n",
        "        # Pad sequences to max_length\n",
        "        padded_sequences = pad_sequences(\n",
        "            [seq for seq in sequences],\n",
        "            maxlen=max_length,\n",
        "            padding='post',\n",
        "            dtype='float32',\n",
        "            value=0.0\n",
        "        )\n",
        "\n",
        "        return padded_sequences\n",
        "\n",
        "    # Process reviews in batches to utilize GPU efficiently\n",
        "    def process_in_batches(df, batch_size=128, max_length=100):\n",
        "        all_sequences = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i in range(0, len(df), batch_size):\n",
        "            batch_texts = df['review_processed'].iloc[i:i+batch_size].values\n",
        "            batch_sequences = create_sequence_vectors(batch_texts, model, max_length)\n",
        "            all_sequences.append(batch_sequences)\n",
        "\n",
        "            # Print progress\n",
        "            if (i+batch_size) % 1000 == 0 or i+batch_size >= len(df):\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"Processed {i+len(batch_texts)}/{len(df)} reviews in {elapsed:.2f} seconds\")\n",
        "\n",
        "        return np.vstack(all_sequences)\n",
        "\n",
        "    # Create sequence vectors with GPU acceleration\n",
        "    MAX_SEQ_LENGTH = 100  # Maximum sequence length\n",
        "    print(\"Creating sequence vectors using GPU...\")\n",
        "    reviews_sequences = process_in_batches(reviews, max_length=MAX_SEQ_LENGTH)\n",
        "    reviews_with_idioms_sequences = process_in_batches(reviews_with_idioms, max_length=MAX_SEQ_LENGTH)\n",
        "\n",
        "# Prepare data for training\n",
        "print(\"Preparing data for CNN training...\")\n",
        "X = reviews_sequences\n",
        "y = reviews['sentiment'].values\n",
        "X2 = reviews_with_idioms_sequences\n",
        "y2 = reviews_with_idioms['sentiment'].values\n",
        "\n",
        "# Convert labels to categorical for neural network\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_cat = to_categorical(y, num_classes=3)  # Assuming 3 sentiment classes (0, 1, 2)\n",
        "y2_cat = to_categorical(y2, num_classes=3)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.4, random_state=12)\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2_cat, test_size=0.4, random_state=12)\n",
        "\n",
        "# Build CNN model\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "def build_cnn_model(input_shape, output_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "    # Add Conv1D layers with different filter sizes to capture different n-gram patterns\n",
        "    model.add(Conv1D(128, 3, activation='relu', input_shape=input_shape, padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    model.add(Conv1D(64, 4, activation='relu', padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # Global max pooling to reduce dimensionality\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "\n",
        "    # Dense layers\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(output_shape, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Train model on reviews only\n",
        "print(\"Training CNN on reviews dataset...\")\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])  # (sequence_length, embedding_dim)\n",
        "output_shape = y_train.shape[1]  # Number of sentiment classes\n",
        "\n",
        "start_time = time.time()\n",
        "cnn_model = build_cnn_model(input_shape, output_shape)\n",
        "history = cnn_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "print(f\"CNN training completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = np.argmax(cnn_model.predict(X_test), axis=1)\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "print(f\"Accuracy for CNN model trained on reviews only: {accuracy:.4f}\")\n",
        "\n",
        "# Train model on reviews + idioms\n",
        "print(\"Training CNN on reviews + idioms dataset...\")\n",
        "input_shape2 = (X2_train.shape[1], X2_train.shape[2])\n",
        "output_shape2 = y2_train.shape[1]\n",
        "\n",
        "start_time = time.time()\n",
        "cnn_model_idioms = build_cnn_model(input_shape2, output_shape2)\n",
        "history2 = cnn_model_idioms.fit(\n",
        "    X2_train, y2_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "print(f\"CNN training completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate model\n",
        "y2_pred = np.argmax(cnn_model_idioms.predict(X_test), axis=1)\n",
        "accuracy2 = accuracy_score(y_test_labels, y2_pred)\n",
        "print(f\"Accuracy for CNN model trained on reviews + idioms: {accuracy2:.4f}\")\n",
        "\n",
        "# Compare model performance\n",
        "print(\"\\nModel Performance Comparison:\")\n",
        "print(f\"Reviews Only:        {accuracy:.4f}\")\n",
        "print(f\"Reviews with Idioms: {accuracy2:.4f}\")\n",
        "print(f\"Improvement:         {(accuracy2-accuracy)*100:.2f}%\")\n",
        "\n",
        "# Save models if needed\n",
        "# cnn_model.save('cnn_model_reviews_kgr10.h5')\n",
        "# cnn_model_idioms.save('cnn_model_reviews_idioms_kgr10.h5')"
      ]
    }
  ]
}