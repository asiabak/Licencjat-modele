{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNueMqPI8mqUTp2i6Mu8qYE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asiabak/Licencjat-modele/blob/main/GRU_by_Claude_with_ftvecotrs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TPxJubbcQGEI",
        "outputId": "881d1a1d-ed36-44b8-ae9e-ffaa9e2c2067"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-GcjystQ8a_",
        "outputId": "e6024df7-b0a7-4b14-8ba9-1eafdf9cf3cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "psbzX62BROyk",
        "outputId": "3b13393d-655f-4db5-9b4a-56d8e4d5bd95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, pandas\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4 pandas-2.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==2.2.2\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3xqvDmNJR6F3",
        "outputId": "11477148-d575-40f6-95d0-739888cf8009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p56Dqh8SP93d",
        "outputId": "e97b1a33-dc75-4827-ce94-425cf83cb081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training GRU on reviews dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.4356 - loss: 1.0508 - val_accuracy: 0.5203 - val_loss: 1.0256\n",
            "Epoch 2/2\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.4991 - loss: 1.0023 - val_accuracy: 0.5075 - val_loss: 1.0160\n",
            "GRU training completed in 6.37 seconds\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "Accuracy for GRU model trained on reviews only: 0.4718\n",
            "Training GRU on reviews + idioms dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4345 - loss: 1.0582 - val_accuracy: 0.4745 - val_loss: 1.0206\n",
            "Epoch 2/2\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.5172 - loss: 1.0069 - val_accuracy: 0.4650 - val_loss: 1.0246\n",
            "GRU training completed in 7.06 seconds\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for GRU model trained on reviews + idioms: 0.4840\n",
            "\n",
            "Model Performance Comparison:\n",
            "Reviews Only:        0.4718\n",
            "Reviews with Idioms: 0.4840\n",
            "Improvement:         1.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# The initial FastText model loading and text preprocessing remains the same\n",
        "# First, verify GPU is available in Colab\n",
        "import tensorflow as tf\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Install necessary libraries if not already available\n",
        "# !pip install gensim scikit-learn nltk pandas\n",
        "\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import FastText\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding, Input, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Use GPU for tensor operations\n",
        "with tf.device('/GPU:0'):\n",
        "    # Download NLTK data if needed\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "    # Set up paths for KGR10 FastText model\n",
        "    model_url = \"https://huggingface.co/clarin-pl/fasttext-kgr10/resolve/main/kgr10.plain.skipgram.dim100.neg10.bin\"\n",
        "    model_path = \"kgr10.plain.skipgram.dim100.neg10.bin\"\n",
        "\n",
        "    # Download model if it doesn't exist\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Downloading KGR10 FastText model from {model_url}...\")\n",
        "        urllib.request.urlretrieve(model_url, model_path)\n",
        "        print(\"Download complete!\")\n",
        "    else:\n",
        "        print(f\"Using existing model at {model_path}\")\n",
        "\n",
        "    # Load the FastText model\n",
        "    print(\"Loading KGR10 FastText model...\")\n",
        "    model = FastText.load_fasttext_format(model_path)\n",
        "    print(f\"Model loaded! Vector size: {model.vector_size}\")\n",
        "\n",
        "    # Load datasets\n",
        "    print(\"Loading datasets...\")\n",
        "    reviews = pd.read_csv(\"filmweb_jednolity_sentyment.csv\")\n",
        "    reviews_with_idioms = pd.read_csv('filmweb_i_idiomy.csv')\n",
        "    print(f\"Loaded {len(reviews)} reviews and {len(reviews_with_idioms)} reviews with idioms\")\n",
        "\n",
        "    # Text preprocessing\n",
        "    def preprocess_text(text):\n",
        "        # Handle NaN values\n",
        "        if isinstance(text, float) and np.isnan(text):\n",
        "            return \"\"\n",
        "\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        words = text.split()\n",
        "        return ' '.join(words)\n",
        "\n",
        "    # Apply text preprocessing\n",
        "    print(\"Preprocessing text...\")\n",
        "    reviews['review_processed'] = reviews['review'].apply(preprocess_text)\n",
        "    reviews_with_idioms['review_processed'] = reviews_with_idioms['review'].apply(preprocess_text)\n",
        "\n",
        "    # Modified vectorization for GRU: We need sequences of word vectors instead of averaging\n",
        "    def create_sequence_vectors(texts, model, max_length=100):\n",
        "        # Get sequence of vectors for each text\n",
        "        def get_sequence_embeddings(text):\n",
        "            if not text:  # Handle empty strings\n",
        "                return [np.zeros(model.vector_size)]\n",
        "\n",
        "            words = word_tokenize(text.lower())\n",
        "            word_vectors = []\n",
        "            for word in words[:max_length]:  # Limit sequence length\n",
        "                try:\n",
        "                    # Get vector for the word\n",
        "                    word_vectors.append(model.wv[word])\n",
        "                except KeyError:\n",
        "                    # Skip words not in vocabulary\n",
        "                    continue\n",
        "\n",
        "            if not word_vectors:\n",
        "                return [np.zeros(model.vector_size)]\n",
        "            return word_vectors\n",
        "\n",
        "        # Process each text in the batch\n",
        "        sequences = [get_sequence_embeddings(text) for text in texts]\n",
        "\n",
        "        # Pad sequences to max_length\n",
        "        padded_sequences = pad_sequences(\n",
        "            [seq for seq in sequences],\n",
        "            maxlen=max_length,\n",
        "            padding='post',\n",
        "            dtype='float32',\n",
        "            value=0.0\n",
        "        )\n",
        "\n",
        "        return padded_sequences\n",
        "\n",
        "    # Process reviews in batches to utilize GPU efficiently\n",
        "    def process_in_batches(df, batch_size=128, max_length=100):\n",
        "        all_sequences = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i in range(0, len(df), batch_size):\n",
        "            batch_texts = df['review_processed'].iloc[i:i+batch_size].values\n",
        "            batch_sequences = create_sequence_vectors(batch_texts, model, max_length)\n",
        "            all_sequences.append(batch_sequences)\n",
        "\n",
        "            # Print progress\n",
        "            if (i+batch_size) % 1000 == 0 or i+batch_size >= len(df):\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"Processed {i+len(batch_texts)}/{len(df)} reviews in {elapsed:.2f} seconds\")\n",
        "\n",
        "        return np.vstack(all_sequences)\n",
        "\n",
        "    # Create sequence vectors with GPU acceleration\n",
        "    MAX_SEQ_LENGTH = 100  # Maximum sequence length\n",
        "    print(\"Creating sequence vectors using GPU...\")\n",
        "    reviews_sequences = process_in_batches(reviews, max_length=MAX_SEQ_LENGTH)\n",
        "    reviews_with_idioms_sequences = process_in_batches(reviews_with_idioms, max_length=MAX_SEQ_LENGTH)\n",
        "\n",
        "# Prepare data for training\n",
        "print(\"Preparing data for GRU training...\")\n",
        "X = reviews_sequences\n",
        "y = reviews['sentiment'].values\n",
        "X2 = reviews_with_idioms_sequences\n",
        "y2 = reviews_with_idioms['sentiment'].values\n",
        "\n",
        "# Convert labels to categorical for neural network\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_cat = to_categorical(y, num_classes=3)  # Assuming 3 sentiment classes (0, 1, 2)\n",
        "y2_cat = to_categorical(y2, num_classes=3)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.4, random_state=12)\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2_cat, test_size=0.4, random_state=12)\n",
        "\n",
        "# Build GRU model\n",
        "def build_gru_model(input_shape, output_shape):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(128, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(GRU(64))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(output_shape, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Train model on reviews only\n",
        "print(\"Training GRU on reviews dataset...\")\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])  # (sequence_length, embedding_dim)\n",
        "output_shape = y_train.shape[1]  # Number of sentiment classes\n",
        "\n",
        "start_time = time.time()\n",
        "gru_model = build_gru_model(input_shape, output_shape)\n",
        "history = gru_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=2,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "print(f\"GRU training completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = np.argmax(gru_model.predict(X_test), axis=1)\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "print(f\"Accuracy for GRU model trained on reviews only: {accuracy:.4f}\")\n",
        "\n",
        "# Train model on reviews + idioms\n",
        "print(\"Training GRU on reviews + idioms dataset...\")\n",
        "input_shape2 = (X2_train.shape[1], X2_train.shape[2])\n",
        "output_shape2 = y2_train.shape[1]\n",
        "\n",
        "start_time = time.time()\n",
        "gru_model_idioms = build_gru_model(input_shape2, output_shape2)\n",
        "history2 = gru_model_idioms.fit(\n",
        "    X2_train, y2_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=2,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "print(f\"GRU training completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate model\n",
        "y2_pred = np.argmax(gru_model_idioms.predict(X_test), axis=1)\n",
        "accuracy2 = accuracy_score(y_test_labels, y2_pred)\n",
        "print(f\"Accuracy for GRU model trained on reviews + idioms: {accuracy2:.4f}\")\n",
        "\n",
        "# Compare model performance\n",
        "print(\"\\nModel Performance Comparison:\")\n",
        "print(f\"Reviews Only:        {accuracy:.4f}\")\n",
        "print(f\"Reviews with Idioms: {accuracy2:.4f}\")\n",
        "print(f\"Improvement:         {(accuracy2-accuracy)*100:.2f}%\")\n",
        "\n",
        "# Save models if needed\n",
        "gru_model.save('gru_model_reviews_kgr10.h5')\n",
        "gru_model_idioms.save('gru_model_reviews_idioms_kgr10.h5')"
      ]
    }
  ]
}