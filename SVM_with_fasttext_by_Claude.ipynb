{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrpO6ggu1F3HDjxd9ROafk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asiabak/Licencjat-modele/blob/main/SVM_with_fasttext_by_Claude.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, verify GPU is available in Colab\n",
        "import tensorflow as tf\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Install necessary libraries if not already available\n",
        "# !pip install gensim scikit-learn nltk pandas\n",
        "\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import FastText\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Use GPU for tensor operations\n",
        "with tf.device('/GPU:0'):\n",
        "    # Download NLTK data if needed\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "    # Set up paths for KGR10 FastText model\n",
        "    model_url = \"https://huggingface.co/clarin-pl/fasttext-kgr10/resolve/main/kgr10.plain.skipgram.dim100.neg10.bin\"\n",
        "    model_path = \"kgr10.plain.skipgram.dim100.neg10.bin\"\n",
        "\n",
        "    # Download model if it doesn't exist\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Downloading KGR10 FastText model from {model_url}...\")\n",
        "        urllib.request.urlretrieve(model_url, model_path)\n",
        "        print(\"Download complete!\")\n",
        "    else:\n",
        "        print(f\"Using existing model at {model_path}\")\n",
        "\n",
        "    # Load the FastText model\n",
        "    print(\"Loading KGR10 FastText model...\")\n",
        "    model = FastText.load_fasttext_format(model_path)\n",
        "    print(f\"Model loaded! Vector size: {model.vector_size}\")\n",
        "\n",
        "    # Load datasets\n",
        "    print(\"Loading datasets...\")\n",
        "    reviews = pd.read_csv(\"filmweb_jednolity_sentyment.csv\")\n",
        "    reviews_with_idioms = pd.read_csv('filmweb_i_idiomy.csv')\n",
        "    print(f\"Loaded {len(reviews)} reviews and {len(reviews_with_idioms)} reviews with idioms\")\n",
        "\n",
        "    # Text preprocessing\n",
        "    def preprocess_text(text):\n",
        "        # Handle NaN values\n",
        "        if isinstance(text, float) and np.isnan(text):\n",
        "            return \"\"\n",
        "\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        words = text.split()\n",
        "        return ' '.join(words)\n",
        "\n",
        "    # Apply text preprocessing\n",
        "    print(\"Preprocessing text...\")\n",
        "    reviews['review_processed'] = reviews['review'].apply(preprocess_text)\n",
        "    reviews_with_idioms['review_processed'] = reviews_with_idioms['review'].apply(preprocess_text)\n",
        "\n",
        "    # GPU-accelerated vectorization with TensorFlow\n",
        "    def vectorize_review_batch(texts, model):\n",
        "        # Function to get embeddings for one text\n",
        "        def get_embeddings(text):\n",
        "            if not text:  # Handle empty strings\n",
        "                return np.zeros(model.vector_size)\n",
        "\n",
        "            words = word_tokenize(text.lower())\n",
        "            word_vectors = []\n",
        "            for word in words:\n",
        "                try:\n",
        "                    # Get vector for the word\n",
        "                    word_vectors.append(model.wv[word])\n",
        "                except KeyError:\n",
        "                    # Skip words not in vocabulary\n",
        "                    continue\n",
        "\n",
        "            if word_vectors:\n",
        "                # Convert to tensor and compute mean\n",
        "                vectors_tensor = tf.convert_to_tensor(word_vectors, dtype=tf.float32)\n",
        "                return tf.reduce_mean(vectors_tensor, axis=0).numpy()\n",
        "            else:\n",
        "                return np.zeros(model.vector_size)\n",
        "\n",
        "        # Process each text in the batch\n",
        "        return [get_embeddings(text) for text in texts]\n",
        "\n",
        "    # Process reviews in batches to utilize GPU efficiently\n",
        "    def process_in_batches(df, batch_size=128):\n",
        "        vectors = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i in range(0, len(df), batch_size):\n",
        "            batch_texts = df['review_processed'].iloc[i:i+batch_size].values\n",
        "            batch_vectors = vectorize_review_batch(batch_texts, model)\n",
        "            vectors.extend(batch_vectors)\n",
        "\n",
        "            # Print progress\n",
        "            if (i+batch_size) % 1000 == 0 or i+batch_size >= len(df):\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"Processed {i+len(batch_texts)}/{len(df)} reviews in {elapsed:.2f} seconds\")\n",
        "\n",
        "        return vectors\n",
        "\n",
        "    # Create vectors with GPU acceleration\n",
        "    print(\"Vectorizing reviews using GPU...\")\n",
        "    reviews_vectors = process_in_batches(reviews)\n",
        "    reviews_with_idioms_vectors = process_in_batches(reviews_with_idioms)\n",
        "\n",
        "    # Store vectors in the dataframes\n",
        "    reviews['vector'] = reviews_vectors\n",
        "    reviews_with_idioms['vector'] = reviews_with_idioms_vectors\n",
        "\n",
        "# Prepare data for training (back on CPU for scikit-learn)\n",
        "print(\"Preparing data for SVM training...\")\n",
        "X = np.vstack(reviews['vector'].values)\n",
        "y = reviews['sentiment'].values\n",
        "X2 = np.vstack(reviews_with_idioms['vector'].values)\n",
        "y2 = reviews_with_idioms['sentiment'].values\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=12)\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.4, random_state=12)\n",
        "\n",
        "# Train model on reviews only\n",
        "print(\"Training SVM on reviews dataset...\")\n",
        "start_time = time.time()\n",
        "model_svc = SVC(kernel='linear')\n",
        "model_svc.fit(X_train, y_train)\n",
        "print(f\"SVM training completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model_svc.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy for model trained on reviews only: {accuracy}\")\n",
        "\n",
        "# Train model on reviews + idioms\n",
        "print(\"Training SVM on reviews + idioms dataset...\")\n",
        "start_time = time.time()\n",
        "model_svc_idioms = SVC(kernel='linear')\n",
        "model_svc_idioms.fit(X2_train, y2_train)\n",
        "print(f\"SVM training completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Predict and evaluate\n",
        "y2_pred = model_svc_idioms.predict(X2_test)\n",
        "accuracy2 = accuracy_score(y2_test, y2_pred)\n",
        "print(f\"Accuracy for model trained on reviews + idioms: {accuracy2}\")\n",
        "\n",
        "# Compare model performance\n",
        "print(\"\\nModel Performance Comparison:\")\n",
        "print(f\"Reviews Only:        {accuracy:.4f}\")\n",
        "print(f\"Reviews with Idioms: {accuracy2:.4f}\")\n",
        "print(f\"Improvement:         {(accuracy2-accuracy)*100:.2f}%\")\n",
        "\n",
        "# Save models if needed\n",
        "import joblib\n",
        "joblib.dump(model_svc, 'svm_model_reviews_kgr10.pkl')\n",
        "joblib.dump(model_svc_idioms, 'svm_model_reviews_idioms_kgr10.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUkwVcjtUEj-",
        "outputId": "8418e01f-b0d7-46f9-82bb-06e490086601"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Downloading KGR10 FastText model from https://huggingface.co/clarin-pl/fasttext-kgr10/resolve/main/kgr10.plain.skipgram.dim100.neg10.bin...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete!\n",
            "Loading KGR10 FastText model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2949133865e3>:41: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
            "  model = FastText.load_fasttext_format(model_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded! Vector size: 100\n",
            "Loading datasets...\n",
            "Loaded 3903 reviews and 4403 reviews with idioms\n",
            "Preprocessing text...\n",
            "Vectorizing reviews using GPU...\n",
            "Processed 3903/3903 reviews in 33.63 seconds\n",
            "Processed 4403/4403 reviews in 34.38 seconds\n",
            "Preparing data for SVM training...\n",
            "Training SVM on reviews dataset...\n",
            "SVM training completed in 0.41 seconds\n",
            "Accuracy for model trained on reviews only: 0.5627400768245838\n",
            "Training SVM on reviews + idioms dataset...\n",
            "SVM training completed in 0.43 seconds\n",
            "Accuracy for model trained on reviews + idioms: 0.5192962542565267\n",
            "\n",
            "Model Performance Comparison:\n",
            "Reviews Only:        0.5627\n",
            "Reviews with Idioms: 0.5193\n",
            "Improvement:         -4.34%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['svm_model_reviews_idioms_kgr10.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}